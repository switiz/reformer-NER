{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2021 12:08:09 - INFO - __main__ -   device: cuda n_gpu: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1ec97eec4f0>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler)\n",
    "import random\n",
    "import numpy as np\n",
    "from finetuning.ner.data_loader import load_and_cache_examples, convert_examples_to_features, get_labels\n",
    "from transformers import BertTokenizer\n",
    "from util.arg import ModelConfig\n",
    "from model.mlm_ner import ReformerNERModel\n",
    "from finetuning.ner.utils import compute_metrics, show_report, get_test_texts\n",
    "from finetuning.ner.data_loader import InputFeatures\n",
    "import tqdm\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "class DecoderFromNamedEntitySequence():\n",
    "    def __init__(self, tokenizer, index_to_ner):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.index_to_ner = index_to_ner\n",
    "\n",
    "    def __call__(self, input_text, list_of_pred_ids):\n",
    "        input_token = input_text\n",
    "        pred_ner_tag = list_of_pred_ids\n",
    "        print(len(input_token), len(pred_ner_tag))\n",
    "        # ----------------------------- parsing list_of_ner_word ----------------------------- #\n",
    "        list_of_ner_word = []\n",
    "        entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "        for i in range(len(input_token)):\n",
    "            pred_ner_tag_str = pred_ner_tag[i]\n",
    "            if \"-B\" in pred_ner_tag_str:\n",
    "                entity_tag = pred_ner_tag_str[:3]\n",
    "                if prev_entity_tag != entity_tag and prev_entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\": entity_word.replace(\"▁\", \" \"), \"tag\": prev_entity_tag, \"prob\": None})\n",
    "                entity_word = input_token[i]\n",
    "                prev_entity_tag = entity_tag\n",
    "            elif entity_tag+\"-I\" in pred_ner_tag_str:\n",
    "                entity_word += input_token[i]\n",
    "            else:\n",
    "                if entity_word != \"\" and entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\":entity_word.replace(\"▁\", \" \"), \"tag\":entity_tag, \"prob\":None})\n",
    "                entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "        # ----------------------------- parsing decoding_ner_sentence ----------------------------- #\n",
    "        decoding_ner_sentence = \"\"\n",
    "        is_prev_entity = True\n",
    "        prev_entity_tag = \"\"\n",
    "        is_there_B_before_I = False\n",
    "\n",
    "        for i, (token_str, pred_ner_tag_str) in enumerate(zip(input_token, pred_ner_tag)):\n",
    "            if i == 0 or i == len(pred_ner_tag)-1: # remove [CLS], [SEP]\n",
    "                continue\n",
    "            token_str = token_str.replace('▁', ' ')  # '▁' 토큰을 띄어쓰기로 교체\n",
    "            print(decoding_ner_sentence)\n",
    "            if '-B' in pred_ner_tag_str:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>'\n",
    "\n",
    "                if token_str[0] == ' ':\n",
    "                    token_str = list(token_str)\n",
    "                    token_str[0] = ' <'\n",
    "                    token_str = ''.join(token_str)\n",
    "                    decoding_ner_sentence += token_str\n",
    "                else:\n",
    "                    decoding_ner_sentence += '<' + token_str\n",
    "                is_prev_entity = True\n",
    "                prev_entity_tag = pred_ner_tag_str[:3] # 첫번째 예측을 기준으로 하겠음\n",
    "                is_there_B_before_I = True\n",
    "\n",
    "            elif '-I' in pred_ner_tag_str:\n",
    "                decoding_ner_sentence += token_str\n",
    "\n",
    "                if is_there_B_before_I is True: # I가 나오기전에 B가 있어야하도록 체크\n",
    "                    is_prev_entity = True\n",
    "            else:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>' + token_str\n",
    "                    is_prev_entity = False\n",
    "                    is_there_B_before_I = False\n",
    "                else:\n",
    "                    decoding_ner_sentence += token_str\n",
    "\n",
    "        return list_of_ner_word, decoding_ner_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def token2id(input_texts,\n",
    "             max_seq_len,\n",
    "             tokenizer,\n",
    "             pad_token_label_id=-100,\n",
    "             cls_token_segment_id=0,\n",
    "             pad_token_segment_id=0,\n",
    "             sequence_a_segment_id=0,\n",
    "             mask_padding_with_zero=True):\n",
    "\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    ids=[]\n",
    "    masks = []\n",
    "    tokenss = []\n",
    "    for input_text in input_texts:\n",
    "        # Tokenize word by word (for NER)\n",
    "        tokens = []\n",
    "        word_tokens = tokenizer.tokenize(input_text)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        ids.append(input_ids)\n",
    "        masks.append(attention_mask)\n",
    "        tokenss.append(tokens)\n",
    "\n",
    "    input_ids = torch.tensor(ids, dtype=torch.long)\n",
    "    attention_mask = torch.tensor(masks, dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_mask, tokenss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def main(input_texts):\n",
    "    logger.info(\"***** INTERACTIVE *****\")\n",
    "    config_path = \"../config/mlm/ner-pretrain-small.json\"\n",
    "    config = ModelConfig(config_path).get_config()\n",
    "    tokenizer = BertTokenizer(vocab_file=config.vocab_path, do_lower_case=False)\n",
    "    input_ids, attention_mask, tokenss = token2id(input_texts, config.max_seq_len, tokenizer)\n",
    "\n",
    "    model = ReformerNERModel(\n",
    "        num_tokens=tokenizer.vocab_size,\n",
    "        dim=config.dim,\n",
    "        depth=config.depth,\n",
    "        heads=config.n_head,\n",
    "        max_seq_len=config.max_seq_len,\n",
    "        causal=False,  # auto-regressive 학습을 위한 설정\n",
    "        num_labels=len(get_labels(config))\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(config.ner_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    logger.info(f'Load Reformer[NER] Model')\n",
    "    logger.info(\"Num features=%s\", len(input_ids))\n",
    "\n",
    "    pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index\n",
    "    model.eval()\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    logger.info(\"Start evaluating!\")\n",
    "    label_lst = get_labels(config)\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
    "        logits = model(input_ids)\n",
    "    preds = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Slot result\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    slot_label_map = {i: label for i, label in enumerate(label_lst)}\n",
    "    preds_list = [[] for _ in range(input_ids.shape[0])]\n",
    "\n",
    "    for i in range(len(input_texts)):\n",
    "        for j in range(len(input_texts[i])):\n",
    "            if input_ids[i, j] != pad_token_label_id:\n",
    "                preds_list[i].append(slot_label_map[preds[i][j]])\n",
    "    labels = get_labels(config)\n",
    "    index_to_ner = {k: v for k, v in enumerate(labels)}\n",
    "    decoder_from_res = DecoderFromNamedEntitySequence(tokenizer=tokenizer, index_to_ner=index_to_ner)\n",
    "    for input_text, preds in zip(input_texts, preds_list):\n",
    "        line = \"\"\n",
    "        for word, pred in zip(input_text, preds):\n",
    "            if pred == 'O':\n",
    "                line = line + word + \" \"\n",
    "            else:\n",
    "                line = line + \"[{}:{}] \".format(word, pred)\n",
    "        print(line)\n",
    "    logger.info(\"***** Eval results *****\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2021 13:04:43 - INFO - __main__ -   ***** INTERACTIVE *****\n",
      "04/10/2021 13:04:43 - INFO - __main__ -   Load Reformer[NER] Model\n",
      "04/10/2021 13:04:43 - INFO - __main__ -   Num features=1\n",
      "04/10/2021 13:04:43 - INFO - __main__ -   Start evaluating!\n",
      "<ipython-input-123-f496df9a343e>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
      "04/10/2021 13:04:43 - INFO - __main__ -   ***** Eval results *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[지:EVT-B] [난:DAT-B] [ :DAT-I] [1:EVT-B] [ :CVL-B] [신:EVT-B] [인:EVT-B] 드 [래:NUM-B] [프:NUM-B] 트 [ :CVL-B] [일:CVL-B] [부:CVL-B] [ :CVL-B] [8:CVL-I] 순 위 로 [ :PER-B] [코:PER-I] [카:PER-B] [콜:PER-I] [라:PER-I] 음 료 [에:CVL-B] [ :CVL-B] [입:ORG-I] [단:ORG-I] [한:ORG-I]   박 성 국 은 [ :PER-B] 파 인 크 리 크   풍 후   아 펜 젤 치 즈 로   출 격 했 다   . \n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "input_text = '지난 1 신인드래프트 일부 8순위로 코카콜라음료에 입단한 박성국은 파인크리크 풍후 아펜젤치즈로 출격했다 .'\n",
    "input_texts.append(input_text)\n",
    "main(input_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}